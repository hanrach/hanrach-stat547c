% !TEX root = ../main.tex

% Exercises section

\section{Exercises}
\begin{enumerate}
	\item Show that estimate of the expectation of a random vector $\varphi_n(\bm{x})$ is
	\begin{equation}
	\E_{\hat{\pi}_n}[\varphi(\bm{x})] = \sum_{i=1}^{N} W_n^i \varphi_n(\bm{X}^i),
	\end{equation} where
	\begin{equation}\label{eq:weights}
	W_n^i = \dfrac{w(X^i_{1:n})}{\sum_{i=1}^{N} w_n(X^i_{1:n})}.
	\end{equation}
	\textit{Solution:} \\
	\begin{align}
	\E_{\hat{\pi}_n}[\varphi(\bm{x})] &= \int \varphi_n(\bm{x}) \hat{\pi}_n(\bm{x}) d\bm{x} = \int \varphi_n(\bm{x}) \hat{\pi}_n(d\bm{x}) \\
	&= \int \dfrac{w(X^i_{1:n})}{\sum_{i=1}^{N} w_n(X^i_{1:n})} \varphi_n(\bm{x}) \delta_{X^i_{1:n}}(\bm{d\bm{x}})\\
	&= \sum_{i=1}^N \dfrac{w(X^i_{1:n})}{\sum_{i=1}^{N} w_n(X^i_{1:n})} \varphi_n \circ X^i_{1:n} = \sum_{i=1}^N W^i_n \varphi_n(X^i_{1:n}).
	\end{align}
	\item Using Levy's Continuity Theorem, prove Cramer-Wold theorem:
	\begin{theorem}[Cramer Wold Theorem]
		Let $\bm{X}_n = (X_{n1}, X_{n2}, ..., X_{nk})$ and $\bm{X} = (X_1, X_2, ..., X_k)$ be random vectors. If $\bm{t}^T \bm{X}_n \overset{D}{\to} \bm{t}^T \bm{X}$ for each $\bm{t} = (t_1, ..., t_k) \in \bbR^k$, then $\bm{X}_n \overset{D}{\to} \bm{X}$.
	\end{theorem}
	\textit{Solution:} \\
	Suppose. $\bm{t}^T \bm{X}_n \overset{D}{\to} \bm{t}^T \bar{\bm{X}}$. Then, $\E[e^{i\bm{t}^T\bm{X}_n}] \overset{D}{\to} \E[e^{i\bm{t}^T\bm{X}}]$ so the characteristic functions converge. Then by Lev's continuity theorem, $\bm{X}_n \overset{D}{\to} \bm{X}$.
	\item Show that if $X_n = \sqrt{n}(Y_n - \theta)$ and $X \overset{D}{\to} \calN(0,\sigma^2)$, then  $\sqrt{n}g'(\theta)(Y_n - \theta) \overset{D}{\to} \calN(0,g'(\theta)^2\sigma^2)$, as used in the proof of multivariate central limit theorem.\\
	\textit{Solution:} \\
	Suppose $X_n \sim \calN(0,\sigma^2)$ with variance $\sigma^2$. Let $Z_n = g'(\theta)X_n$. Then $\var(Z_n) = \var(g'(\theta)X_n) = g'(\theta)^2\var(X_n) = g'(\theta)^2\sigma^2$. Since the characteristic function determines the distribution, using the fact that $\E[e^{itX_n}] = e^{\frac{1}{2}\sigma^2t^2}$, $\E[e^{itZ_n}] = \E[e^{itg'(\theta)X_n}] = \E[e^{i(tg'(\theta))X_n}] = e^{\frac{1}{2}(tg'(\theta))^2\sigma^2} $. This characteristic function of $Z_n$ is equal to that of $\calN(0,g'(\theta)^2\sigma^2)$. Therefore, $Z_n \sim \calN(0,g'(\theta)^2\sigma^2)$. So if $X_n = \sqrt{n}(Y_n -g(\theta)) \overset{D}{\to} \calN(0,\sigma^2)$ then $Z_n  =  g'(\theta)\sqrt{n}(Y_n - \theta) \overset{D}{\to} \calN(0,g'(\theta)^2\sigma^2)$. 
	\item  Show that $\text{Var}(w_{n}) \leq \text{Var}(w_{n+1})$ as a consequence of Kong-Liu-Wong Theorem.\\
	\textit{Solution:} \\
	The theorem tells us that $\var(w_n) = \var(\E[w_{n+1}|\calF_n]).$ It remains to show that $$\var(\E[w_{n+1}|\calF_n]) \leq \var(w_{n+1}).$$
	Let $(\Omega,\calH,\bbP)$ be our probability space, and let $V$ be a collection of all random variables with finite variance. $V$ is a Hilbert space with under inner product $<X,Y> = \E[XY]$, for $X, Y \in V$ \cite{zimmerman}. So $w_{n+1} \in V$. $\calF_n$ which is sigma algebra generated by a collection random variables in $V$ is a closed subspace of $V$.  $w_{n+1}$ The two variances of interest are given by:
	\begin{align}
	\var(w_{n+1}) &= \E[(w_{n+1})^2] - \E[w_{n+1}]^2, \\
	\var(\E[w_{n+1}|\calF_n]) &= \E[\E[w_{n+1}|\calF_n]^2] - \E[\E[w_{n+1}|\calF_n]]^2 = \E[\E[w_{n+1}|\calF_n]^2] - E[w_{n+1}]^2.
	\end{align}
	It remains to show that
	\begin{equation}
	\E[\E[w_{n+1}|\calF_n]^2] \leq \E[(w_{n+1})^2].
	\end{equation} 
	Writing in terms of inner product,
	\begin{equation}
	\E[\E[w_{n+1}|\calF_n]^2] = \norm{\E[w_{n+1}|\calF_n]}^2
	\end{equation}
	The conditional expectation of $w_{n+1}$ given $\calF_n$ is equivalent to the orthogonal projection of $w_{n+1}$ onto closed subspace $\calF_n$. Let $P:V \to V$ be the projection operator. Then $\E[w_{n+1}|\calF_n] = P_{\calF_n}(w_{n+1}).$ By the property of orthogonal projection,
	\begin{equation}\label{eq:proj}
		\norm{w_{n+1}}^2 = \norm{P_{\calF_n}(w_{n+1})}^2 + \norm{w_{n+1} - P_{\calF_n}(w_{n+1})}^2.
	\end{equation}
	By the projection property \eqref{eq:proj}, $\norm{w_{n+1} - P_{\calF_n}(w_{n+1})}^2 \geq 0$, $\norm{w_{n+1}}^2 = \E[(w_{n+1})^2] \geq norm{P_{\calF_n}(w_{n+1})}^2 = \E[\E[w_{n+1}|\calF_n]^2]$. 
\end{enumerate}
